# 基于LSTM的唐诗语言模型实验报告

## 1. 实验概述

### 1.1 任务描述
基于唐诗语料训练字符级语言模型，实现：
1. 从零随机生成古诗
2. 给定第一句诗，续写剩余诗句
3. 在训练集和测试集上计算困惑度（Perplexity），评估模型性能

### 1.2 实验配置
| 配置项 | 值 |
|-------|-----|
| 训练数据 | poetryFromTang.txt（163 首诗） |
| 测试数据 | poetryTest.txt（10 首诗） |
| 词表大小 | 2503 |
| 模型 | LSTM语言模型 |
| Embedding维度 | 128 |
| 隐藏层维度 | 256 |
| Dropout | 0.5 |
| 学习率 | 0.001（Adam优化器） |
| Batch Size | 64 |
| 训练轮数 | 100 |
| 梯度裁剪 | max_norm=5.0 |
| 设备 | cuda |
| 总训练时间 | 32.87s |

---

## 2. 算法原理

### 2.1 语言模型

语言模型的目标是建模文本序列的概率分布。给定一个字符序列 $w_1, w_2, \ldots, w_T$，语言模型估计其联合概率：

$$P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^{T} P(w_t \mid w_1, \ldots, w_{t-1})$$

训练时，模型在每个时间步根据历史字符预测下一个字符，使用交叉熵损失：

$$L = -\frac{1}{T}\sum_{t=1}^{T}\log P(w_t \mid w_1, \ldots, w_{t-1})$$

### 2.2 LSTM（长短期记忆网络）

标准RNN存在梯度消失问题，难以捕获长距离依赖。LSTM通过门控机制解决此问题：

**遗忘门**（决定丢弃多少旧信息）：

$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

**输入门**（决定写入多少新信息）：

$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$

$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

**记忆单元更新**：

$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$

**输出门**（决定输出多少信息）：

$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$

$$h_t = o_t \odot \tanh(C_t)$$

其中 $\sigma$ 是sigmoid函数，$\odot$ 是逐元素乘法。记忆单元 $C_t$ 通过遗忘门和输入门的线性组合更新，梯度可以沿 $C_t$ 直接传播，缓解了梯度消失问题。

### 2.3 模型结构

本实验的语言模型结构为：

```
输入字符 → Embedding(128维) → Dropout(0.5) → LSTM(256维) → Linear → Softmax → 预测下一个字符
```

- **Embedding层**：将每个字符映射为128维稠密向量，使用Xavier初始化
- **LSTM层**：输入128维，隐藏状态256维，捕获序列上下文
- **全连接层**：将256维隐藏状态映射到词表大小（2503），输出每个字符的logits
- **训练目标**：输入序列为 $[w_1, \ldots, w_{T-1}]$，目标序列为 $[w_2, \ldots, w_T]$，即预测下一个字符

### 2.4 困惑度（Perplexity）

困惑度是语言模型的标准评估指标，定义为交叉熵损失的指数：

$$\text{PPL} = \exp\left(-\frac{1}{T}\sum_{t=1}^{T}\log P(w_t \mid w_{<t})\right) = \exp(L)$$

直观含义：模型在每个时间步平均"犹豫"多少个候选字符。PPL越低说明模型对文本的预测越准确：
- PPL = 1：模型完美预测每个字符
- PPL = V（词表大小）：等价于均匀随机猜测

### 2.5 解码策略：Temperature采样

生成诗歌时，需要从模型输出的概率分布中选择下一个字符。

**贪心解码**（temperature=0）：始终选择概率最高的字符，容易陷入重复循环（如反复生成逗号）。

**Temperature采样**：

$$P'(w) = \frac{\exp(z_w / \tau)}{\sum_j \exp(z_j / \tau)}$$

其中 $z_w$ 是logits，$\tau$ 是temperature参数：
- $\tau \to 0$：趋近贪心解码，输出确定但缺乏多样性
- $\tau = 1$：按模型原始概率采样
- $\tau > 1$：分布更平坦，输出更随机

本实验使用 $\tau = 0.8$，在多样性和质量之间取得平衡。

### 2.6 未登录词（OOV）处理

测试集中可能出现训练集词表之外的字符。本实验引入 `<unk>` 特殊标记（id=3），将所有OOV字符映射到该标记。

由于 `<unk>` 在训练数据中从未出现，其embedding未被有效训练，因此：
- 作为输入时，产生的隐状态质量较差，影响后续预测
- 作为预测目标时，模型几乎不会给它高概率，产生较大loss
- OOV比例越高，测试集困惑度越会偏高

---

## 3. 实验结果

### 3.1 数据统计

| 项目 | 数值 |
|------|------|
| 训练集诗歌数 | 163 |
| 测试集诗歌数 | 10 |
| 词表大小 | 2503 |
| 测试集总字符数 | 3536 |
| OOV字符数 | 201 |
| OOV比例 | 5.68% |

### 3.2 训练过程（每10个epoch）

| Epoch | Train Loss | Train PPL | Test Loss | Test PPL |
|-------|-----------|-----------|----------|----------|
| 10 | 6.4068 | 605.97 | 7.0476 | 1150.13 |
| 20 | 6.2952 | 541.95 | 7.1710 | 1301.10 |
| 30 | 6.2735 | 530.35 | 7.2495 | 1407.37 |
| 40 | 6.2307 | 508.11 | 7.2853 | 1458.74 |
| 50 | 6.1391 | 463.64 | 7.2680 | 1433.73 |
| 60 | 6.0026 | 404.48 | 7.2066 | 1348.32 |
| 70 | 5.8007 | 330.54 | 7.1284 | 1246.90 |
| 80 | 5.5927 | 268.47 | 7.1281 | 1246.46 |
| 90 | 5.3951 | 220.33 | 7.2287 | 1378.37 |
| 100 | 5.2059 | 182.34 | 7.3478 | 1552.81 |


### 3.3 最终困惑度

| 数据集 | Loss | PPL |
|--------|------|-----|
| 训练集 | 5.2059 | 182.34 |
| 测试集 | 7.3478 | 1552.81 |

### 3.4 训练曲线

![训练曲线](training_curve.png)

### 3.5 训练过程中生成的诗歌

**Epoch 10：**

```
沃秦禅冠氓待日来。
山。
。
。
```

**Epoch 20：**

```
熊泽佳径客期禁区。
。
逸郎两，。
。
```

**Epoch 30：**

```
央暮盈难处歌喙不得独露
言。
龙，。
，谁可凉，憔，
```

**Epoch 40：**

```
春攀远地，其，，，前哭鼓
楚云事物。
立驱潭宅。
，吾云以雪台，之假，得逆。
```

**Epoch 50：**

```
温龙日未人闻，，林。
见，云酒
亦鹰空干日白与。
地飞，泉犹荷。
```

**Epoch 60：**

```
论远旧，，缺朝人策闲园，金，浔数。
鶺。
。
舞，进，子洛尽，。
```

**Epoch 70：**

```
柳渡龙斋前过。
，闻入杯，城。
寒。
来，之，。
```

**Epoch 80：**

```
扫谢，榻旧人几，山宰，眷岁手何，别寒。
黄，。
君军，山江，山楚，寒朝，管所前面文外。
推朝，愿喜人故里，上过。
```

**Epoch 90：**

```
芳瑞十居，南玉，府辕传，日开。
云流。
影血越溯，夜石如间。
何雁来闲。
```

**Epoch 100：**

```
故家山登庭时，来侯。
古，家临。
月接山。
寒上，新
```



### 3.6 最终随机生成诗歌

```
事台华多。
，念度，开饮黄唱，朝寒远，朱之香奚客，头，帟蠡未泥成，灶君。
一郭，
来处，筵见。
```

### 3.7 续写生成结果

**输入：** `床前明月光，`

```
床前明月光，
时车里，不月鱼，间亦飞唱如游。
遥饵堆旧，星见，扬邮烟处，行之乔。
愿寒。
```

**输入：** `白日依山尽，`

```
白日依山尽，
十之销。
柏戎，金熳非汉惯。
岁梁疑书其郎哭，
```

**输入：** `春眠不觉晓，`

```
春眠不觉晓，
堆自台。
无长，南海
```



---

## 4. 实验结论

### 4.1 生成质量分析

从生成结果来看，模型的生成质量很差，存在以下明显问题：
- **语义混乱**：生成的文本是无意义的字符拼凑，如"沃秦禅冠氓待日来"、"熊泽佳径客期禁区"，不构成有效的语义
- **格式不稳定**：句子长度参差不齐，频繁出现孤立的标点符号（如单独的"。"或"，。"），未能学会五言或七言的固定句式
- **续写失败**：给定"床前明月光，"等经典首句，续写结果完全不通顺，与首句无语义关联

训练集PPL最终仍高达182.34，意味着模型在每个时间步平均在182个候选字符中犹豫，远未收敛到可用水平。

### 4.2 生成质量差的原因

**根本原因是训练数据严重不足**：

| 问题 | 具体数据 | 影响 |
|------|---------|------|
| 训练集太小 | 仅163首诗 | 模型见过的字符序列模式极其有限，无法学到通用的语言规律 |
| 词表稀疏 | 2503字，但大量低频字只出现1-2次 | 低频字的embedding无法被充分训练 |
| 样本/参数比失衡 | 训练文本约2万字符，但模型参数远超此数 | 模型容量远大于数据量，极易过拟合 |

其他原因：
- **单层LSTM容量有限**：即使数据充足，单层256维LSTM也难以建模古诗的复杂结构（对仗、押韵、用典）
- **字符级建模粒度过细**：逐字符预测需要模型隐式学习词语边界和语法结构，在数据不足时尤其困难

### 4.3 训练集与测试集困惑度差距

| 数据集 | Loss | PPL |
|--------|------|-----|
| 训练集 | 5.2059 | 182.34 |
| 测试集 | 7.3478 | 1552.81 |

测试集PPL是训练集的8.5倍，差距悬殊，原因包括：
1. **严重过拟合**：从训练曲线可以看到，训练集Loss持续下降，但测试集Loss在Epoch 70-80之后反而上升，典型的过拟合表现
2. **OOV影响**：测试集有201个未登录词（5.68%），这些字符被映射为未经训练的`<unk>`，直接推高了困惑度
3. **分布不匹配**：测试集选取了李白、杜甫、白居易的长诗，风格和用字与训练集差异较大

### 4.4 贪心解码 vs Temperature采样

| 解码策略 | 实际表现 |
|---------|---------|
| 贪心（temperature=0） | 完全不可用，陷入死循环反复输出逗号"，" |
| Temperature=0.8 | 避免了死循环，但生成内容仍是无意义的字符拼凑 |

贪心解码失败说明模型学到的概率分布高度集中在逗号等高频标点上，模型并未真正学会语言结构。Temperature采样只是掩盖了这个问题，通过引入随机性强制输出多样化，但并未改善语义质量。

### 4.5 续写功能分析

续写功能的实现思路是正确的——将首句逐字符喂入LSTM积累隐状态，再自回归生成后续内容。但由于模型本身质量太差，续写结果与首句无语义关联，实际不可用。

---

## 5. 总结

1. **本实验的核心瓶颈是数据量不足**：163首诗（约2万字符）远远不够训练一个字符级语言模型。参考相关工作，通常需要数万首诗（如全唐诗约4.8万首）才能获得基本可用的生成效果
2. **困惑度指标**忠实地反映了模型质量：训练集PPL 182和测试集PPL 1553都表明模型未能有效学习语言规律
3. **OOV问题**在小词表场景下影响显著，5.68%的OOV率进一步恶化了测试集困惑度
4. **贪心解码的失败**暴露了模型概率分布的不合理性，Temperature采样只能缓解而非解决问题
5. **改进方向**：
   - **最关键：扩充训练数据**，使用完整的全唐诗语料（4.8万首）或加入宋词等数据
   - 增加LSTM层数（2-3层），提升模型容量
   - 引入Attention机制增强长距离依赖
   - 尝试Transformer架构
   - 添加学习率衰减和早停策略防止过拟合
