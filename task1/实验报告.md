# Softmax回归情感分析实验报告

## 1. 实验概述

### 1.1 任务描述
基于电影评论数据集进行5分类情感分析任务，使用Softmax回归模型，探究不同N-gram特征、批量大小、采样策略和学习率对模型性能的影响。

### 1.2 实验配置
- 数据集: 电影评论数据，共15278条训练样本
- 训练/测试划分: 98% / 2%
- 模型: Softmax多分类回归
- 训练轮数: 100个epoch（完整遍历数据集100次）

---

## 2. 算法原理

### 2.1 Softmax回归

Softmax回归是逻辑回归在多分类问题上的推广。

**前向传播**:

线性变换：

$$o = W \cdot x$$

其中 $W \in \mathbb{R}^{K \times D}$ 是权重矩阵，$K=5$ 是类别数，$D$ 是特征维度。

Softmax激活函数将输出转换为概率分布：

$$\hat{y}_k = \text{softmax}(o)_k = \frac{e^{o_k}}{\sum_{j=1}^{K} e^{o_j}}$$

**数值稳定性处理**（防止exp溢出）:

$$o' = o - \max(o)$$

```python
o = o - numpy.max(o, axis=0, keepdims=True)
o = numpy.exp(o)
y_hat = o / numpy.sum(o, axis=0, keepdims=True)
```

**损失函数（交叉熵）**:

$$L = -\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1}^{K} y_{ik} \log(\hat{y}_{ik})$$

**梯度计算**:

$$\nabla_W L = \frac{1}{N}(\hat{Y} - Y) \cdot X^T$$

**参数更新（梯度下降）**:

$$W \leftarrow W - \eta \cdot \nabla_W L$$

其中 $\eta$ 是学习率，$N$ 是batch_size。

关键点：梯度除以batch_size取平均，因此不同batch_size下梯度量级相近，但方差不同。

### 2.2 N-gram词袋模型

N-gram将连续N个词组合成特征单元：

| N-gram类型 | 示例句子"I love this" | 特征 |
|-----------|---------------------|------|
| 1-gram | 单词 | "I", "love", "this" |
| 2-gram | 相邻词对 | "I_love", "love_this" |
| 3-gram | 相邻三词 | "I_love_this" |

实验中的特征维度：
- 1-gram: 3,737维
- (1,2)-gram: 13,553维（包含1-gram和2-gram）
- (1,2,3)-gram: 24,805维（包含1/2/3-gram）

### 2.3 批量采样策略

**随机采样（Shuffle）**:
```python
idx = random.randint(0, n_samples, size=batch_size)  # 有放回随机采样
x = train_x[:, idx]
```
- 每次随机抽取batch_size个样本
- 样本可能重复，引入随机噪声
- 有助于跳出局部最优

**顺序遍历（Sequential）**:
```python
x = train_x[:, start:start+batch_size]  # 按顺序取
start = (start + batch_size) % n_samples
```
- 按固定顺序依次取样本
- 每个epoch恰好遍历一次完整数据
- 梯度方向相对固定

### 2.4 学习率与批量大小的关系

梯度估计的方差与batch_size的关系：

$$\text{Var}(\nabla L) \propto \frac{\sigma^2}{N}$$

其中 $N$ 是batch_size，$\sigma^2$ 是单样本梯度的方差。

因此：
- 小batch：梯度估计方差大（$N$小），需要小学习率防止震荡
- 大batch：梯度估计稳定（$N$大），可用大学习率加速收敛

**线性缩放规则**：当batch_size增大 $k$ 倍时，学习率也应增大约 $k$ 倍：

$$\eta_{new} \approx k \cdot \eta_{old}$$

实验中的学习率配置：
| Batch Size | 学习率范围 |
|------------|-----------|
| 1 (SGD) | 0.01, 0.05, 0.1, 0.2 |
| 16 | 0.05, 0.1, 0.2, 0.5 |
| 32 | 0.1, 0.2, 0.5, 1.0 |
| 64 | 0.2, 0.5, 1.0, 5.0 |

---

## 3. 实验结果

### 3.1 各配置最佳测试准确率

| N-gram | Shuffle_SGD | Shuffle_16 | Shuffle_32 | Shuffle_64 | Seq_1 | Seq_16 | Seq_32 | Seq_64 |
|--------|-------------|------------|------------|------------|-------|--------|--------|--------|
| 1-gram | 0.537 | 0.522 | 0.528 | 0.528 | 0.522 | 0.522 | 0.528 | 0.528 |
| (1,2)-gram | 0.559 | 0.525 | **0.571** | 0.537 | 0.537 | 0.540 | 0.556 | 0.553 |
| (1,2,3)-gram | 0.578 | 0.547 | 0.571 | 0.543 | **0.581** | 0.540 | 0.565 | 0.578 |

### 3.2 各N-gram最佳配置

| N-gram | 特征数 | 最佳配置 | 最佳学习率 | 测试准确率 | 训练准确率 |
|--------|-------|---------|-----------|-----------|-----------|
| 1-gram | 3,737 | Shuffle_SGD | 0.1 | 53.73% | 73.77% |
| (1,2)-gram | 13,553 | Shuffle_32 | 1.0 | 57.14% | 93.13% |
| (1,2,3)-gram | 24,805 | Sequential_1 | 0.2 | 58.07% | 99.91% |

### 3.3 训练时间对比（以1-gram为例）

| 配置 | 单次实验时间 |
|-----|-------------|
| SGD (batch=1) | ~170-240秒 |
| batch=16 | ~55-70秒 |
| batch=32 | ~48-65秒 |
| batch=64 | ~84-90秒 |

---

## 4. 实验结论

### 4.1 N-gram特征的影响

**结论**: 更高阶N-gram能提升测试准确率，但边际收益递减且加剧过拟合。

| 对比 | 测试准确率提升 | 训练准确率 |
|-----|---------------|-----------|
| 1-gram → (1,2)-gram | +3.4% (53.7%→57.1%) | 73%→93% |
| (1,2)-gram → (1,2,3)-gram | +0.9% (57.1%→58.1%) | 93%→99.9% |

**分析**:
- 2-gram能捕获词序信息（如"not good"与"good"的区别），提升明显
- 3-gram带来的额外信息有限，但特征维度增加近一倍
- 训练准确率接近100%说明模型完全记住了训练数据，泛化能力差

### 4.2 Shuffle vs Sequential

**结论**: 两种策略没有绝对优劣，但表现出不同特点。

| N-gram | Shuffle最佳 | Sequential最佳 | 获胜方 |
|--------|------------|---------------|-------|
| 1-gram | 0.537 (SGD) | 0.522 (batch=1) | Shuffle |
| (1,2)-gram | 0.571 (batch=32) | 0.556 (batch=32) | Shuffle |
| (1,2,3)-gram | 0.578 (SGD) | 0.581 (batch=1) | Sequential |

**分析**:
- Shuffle在低维特征(1-gram, (1,2)-gram)上略优
- Sequential在高维特征((1,2,3)-gram)上略优
- 差距不大（<1%），说明对于凸优化问题（Softmax回归），两种策略最终都能收敛到相近的解

### 4.3 Batch Size的影响

**结论**: SGD(batch=1)和中等batch(32)表现最好，大batch(64)效果较差。

从热力图可以看出：
- **SGD (batch=1)**: 在所有N-gram上都有竞争力的表现
- **batch=32**: 在(1,2)-gram上达到最佳
- **batch=64**: 整体表现最差

**分析**:
- 小batch的随机噪声有正则化效果，防止过拟合
- 大batch虽然梯度准确，但容易陷入"尖锐极小值"（sharp minima），泛化能力差
- 在严重过拟合的情况下（训练准确率接近100%），小batch的噪声反而有利

### 4.4 学习率的影响

**结论**: 最优学习率随batch size增大而增大，符合线性缩放规则。

各配置的最优学习率：
| Batch Size | 1-gram最优lr | (1,2)-gram最优lr | (1,2,3)-gram最优lr |
|------------|-------------|-----------------|-------------------|
| 1 (SGD) | 0.1 | 0.2 | 0.2 |
| 32 | 0.5-1.0 | 1.0 | 1.0 |
| 64 | 5.0 | 1.0-5.0 | 5.0 |

**分析**:
- batch size增大k倍，最优学习率也大约增大k倍
- 过小的学习率收敛慢，过大的学习率导致震荡
- 大学习率+大batch容易过拟合（训练准确率快速上升但测试准确率停滞）

### 4.5 过拟合问题

**结论**: 实验中存在严重的过拟合现象。

| N-gram | 训练准确率 | 测试准确率 | 差距 |
|--------|-----------|-----------|------|
| 1-gram | 73.8% | 53.7% | 20.1% |
| (1,2)-gram | 93.1% | 57.1% | 36.0% |
| (1,2,3)-gram | 99.9% | 58.1% | 41.8% |

**分析**:
- 特征维度远大于样本数时，模型容易过拟合
- (1,2,3)-gram有24805维特征，但只有15278个样本
- 可以考虑添加L2正则化来缓解过拟合

---

## 5. 总结

1. **N-gram选择**: (1,2)-gram是性价比最高的选择，能捕获词序信息且过拟合相对可控

2. **批量策略**: Shuffle和Sequential差别不大，但Shuffle略稳定

3. **Batch Size**: 推荐使用SGD或batch=32，避免使用过大的batch

4. **学习率**: 需要根据batch size调整，遵循"大batch用大学习率"原则

5. **改进方向**:
   - 添加L2正则化（权重衰减）缓解过拟合
   - 使用更多训练数据
   - 尝试特征选择减少维度
   - 考虑使用交叉验证选择超参数
