# 实验1：词嵌入方式对比

## 实验设置

- **数据集**: Movie Reviews情感分类，5分类
- **模型**: CNN（4种卷积核尺寸：2,3,4,5，每种16个）
- **对比方法**: Random Embedding vs Pre-trained Embedding (GloVe 50d)
- **训练轮数**: 100 epochs
- **Batch Size**: 64

---

## 实验原理

### 词嵌入的作用

词嵌入将离散的词符号映射到连续向量空间：

$$\text{word} \xrightarrow{\text{Embedding}} \mathbf{v} \in \mathbb{R}^d$$

对于词汇表 $V$，建立嵌入矩阵 $\mathbf{E} \in \mathbb{R}^{|V| \times d}$

### Random Embedding

随机初始化，从均匀分布采样：

$$E_{ij} \sim \mathcal{U}(-0.1, 0.1)$$

训练时通过反向传播更新：

$$\mathbf{E}^{(t+1)} = \mathbf{E}^{(t)} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{E}^{(t)}}$$

**特点**:
- 从零开始学习
- 需要足够的数据
- 对任务高度定制

### Pre-trained Embedding

使用大规模语料预训练的词向量（GloVe）：

$$\mathbf{E}_{pre} = \text{GloVe}(\text{Wikipedia + Gigaword})$$

GloVe目标函数：

$$J = \sum_{i,j} f(X_{ij})(\mathbf{w}_i^T \tilde{\mathbf{w}}_j + b_i + \tilde{b}_j - \log X_{ij})^2$$

**特点**:
- 包含通用语义知识
- 词间关系已建模
- 对低频词友好

---

## CNN模型结构

### 输入层
文本序列 → 词嵌入矩阵：

$$\mathbf{X} = [\mathbf{v}_1; \mathbf{v}_2; ...; \mathbf{v}_n] \in \mathbb{R}^{n \times d}$$

### 卷积层
对于卷积核大小 $h$：

$$c_i = \text{ReLU}(\mathbf{W} * \mathbf{X}_{i:i+h-1} + b)$$

使用4种尺寸: $h \in \{2, 3, 4, 5\}$，每种16个卷积核

### 池化层
Max-over-time pooling：

$$\hat{c} = \max\{c_1, c_2, ..., c_{n-h+1}\}$$

### 全连接层

$$\mathbf{y} = W_{fc}[\hat{\mathbf{c}}^{(2)}; \hat{\mathbf{c}}^{(3)}; \hat{\mathbf{c}}^{(4)}; \hat{\mathbf{c}}^{(5)}] + b$$

---

## 关键梯度推导

### Softmax + 交叉熵

损失函数：

$$\mathcal{L} = -\sum_{c=1}^{C} y_c \log(\hat{y}_c)$$

其中 $\hat{y}_c = \frac{\exp(o_c)}{\sum_k \exp(o_k)}$

**梯度推导**:

$$\frac{\partial \mathcal{L}}{\partial o_j} = \frac{\partial}{\partial o_j}\left(-\sum_c y_c \log(\hat{y}_c)\right)$$

利用链式法则和softmax导数：

$$\frac{\partial \hat{y}_i}{\partial o_j} = \begin{cases}\hat{y}_i(1-\hat{y}_i) & i=j \\ -\hat{y}_i\hat{y}_j & i \neq j\end{cases}$$

最终得到：

$$\boxed{\frac{\partial \mathcal{L}}{\partial o_j} = \hat{y}_j - y_j}$$

这个简洁的结果使得反向传播非常高效。

### Max Pooling梯度

Max pooling只保留最大值：

$$\hat{c} = \max_i c_i$$

梯度只传递给最大值位置：

$$\frac{\partial \mathcal{L}}{\partial c_i} = \begin{cases}\frac{\partial \mathcal{L}}{\partial \hat{c}} & i = \arg\max_j c_j \\ 0 & \text{否则}\end{cases}$$

### 卷积层梯度

卷积操作：$c_i = \text{ReLU}(\sum W \cdot X + b)$

ReLU导数：

$$\frac{d\text{ReLU}(x)}{dx} = \mathbb{1}_{x>0}$$

卷积核梯度：

$$\frac{\partial \mathcal{L}}{\partial W} = \sum_i \frac{\partial \mathcal{L}}{\partial c_i} \cdot \mathbb{1}_{c_i>0} \cdot X_i$$

### Embedding层梯度（关键差异）

对于词 $w$ 在位置 $t$，embedding向量 $\mathbf{v}_w = \mathbf{E}[w]$

**Random Embedding**:

$$\frac{\partial \mathcal{L}}{\partial \mathbf{E}[w]} = \sum_{t: w_t=w} \frac{\partial \mathcal{L}}{\partial \mathbf{v}_t}$$

同一个词在不同位置的梯度累加，然后更新：

$$\mathbf{E}[w] \leftarrow \mathbf{E}[w] - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{E}[w]}$$

**Pre-trained Embedding (可选两种策略)**:

1. Frozen: $\frac{\partial \mathcal{L}}{\partial \mathbf{E}_{pre}} = 0$，不更新
2. Fine-tuning: 用较小学习率更新

---

## 实验结果

### 性能对比

| 模型 | 训练时间(秒) | 最终准确率(%) | 最佳准确率(epoch) |
|------|------------|--------------|-----------------|
| Random Embedding | 946.76 | 61.55 | 61.86 (epoch 93) |
| Pre-trained Embedding | 1009.65 | 61.98 | 61.98 (epoch 100) |

**差异**: Pre-trained仅高0.43%，但训练时间多62.89秒

### 学习曲线分析

**初始阶段 (Epoch 1-10)**:
- Random: 35.10% → 48.52%
- Pre-trained: 32.70% → 59.01%

**Pre-trained在前10个epoch快速达到59%，而Random只有48.5%**

**中期阶段 (Epoch 10-50)**:
- Random: 48.52% → 59.52%
- Pre-trained: 59.01% → 62.50%

两者差距在缩小

**后期阶段 (Epoch 50-100)**:
- Random: 59.52% → 61.55%
- Pre-trained: 62.50% → 61.98%

Random持续学习，最终接近Pre-trained

### 训练损失对比

**Epoch 1**:
- Random: 1.018
- Pre-trained: 1.044

**Epoch 50**:
- Random: 0.505
- Pre-trained: 0.571

**Epoch 100**:
- Random: 0.467
- Pre-trained: 0.500

Random的loss最终更低，说明在训练集上拟合更好

---

## 理论分析

### 为什么Pre-trained初期快？

#### 1. 语义先验

Pre-trained embedding已经编码了词间关系：

$$\cos(\mathbf{v}_{good}, \mathbf{v}_{great}) \approx 0.8$$
$$\cos(\mathbf{v}_{good}, \mathbf{v}_{bad}) \approx -0.3$$

Random初始时：

$$\mathbb{E}[\cos(\mathbf{v}_i, \mathbf{v}_j)] \approx 0$$

#### 2. 低频词优势

训练集中低频词（出现<5次）梯度很小，Random难以学习：

$$\left\|\frac{\partial \mathcal{L}}{\partial \mathbf{v}_w}\right\| \approx 0 \text{ when } \text{count}(w) \to 0$$

Pre-trained已包含其语义

#### 3. 优化景观

Pre-trained提供更好的初始化点 $\mathbf{E}_0$：

$$\mathcal{L}(\theta_0, \mathbf{E}_{pre}) < \mathcal{L}(\theta_0, \mathbf{E}_{rand})$$

### 为什么Random最终追上？

#### 1. 任务特定优化

Random embedding完全针对情感分类任务优化，不受通用语义约束

#### 2. 数据充足性

15万训练样本足够学习有效的词表示

#### 3. 过拟合风险

Random最终loss更低但准确率相近，可能存在轻微过拟合

---

## 计算复杂度

### 时间复杂度

**前向传播**:
- Embedding: $O(nd)$
- Convolution: $O(ndhK)$，$K=64$为总卷积核数
- Pooling: $O(nK)$
- FC: $O(KC)$，$C=5$

总计: $O(ndhK)$

**反向传播**: 相同复杂度

**Random vs Pre-trained的差异**:
- Random需要更新embedding: 额外 $O(|V|d)$ 梯度计算
- Pre-trained (frozen): 跳过embedding更新

但实验显示Pre-trained反而更慢（1009秒 vs 947秒），可能原因：
1. PyTorch实现细节
2. Pre-trained模型其他层收敛较慢

### 空间复杂度

- Embedding: $O(|V| \cdot d)$
- Model params: $O(Khd + KC)$
- Activations: $O(bnd)$，$b$为batch size

---

## 关键发现

### 1. 收敛速度

**Pre-trained赢**: 前10个epoch达到59%，Random只有48.5%

快26%的收敛速度对于需要快速迭代的场景很重要

### 2. 最终性能

**平局**: 差异仅0.43%，统计上不显著

说明在足够数据下，Random可以学到同样好的表示

### 3. 训练效率

**Random略胜**: 训练时间少62秒

虽然差异不大，但在大规模训练中累积效应明显

### 4. 过拟合风险

Random的训练loss更低但准确率相近，需要注意正则化

---

## 实际应用建议

### 选择Random Embedding的场景

✅ 数据量充足（>10万样本）
✅ 领域词汇特殊（医疗、法律等）
✅ 追求任务特定优化
✅ 训练时间不敏感

### 选择Pre-trained Embedding的场景

✅ 数据量不足（<5万样本）
✅ 需要快速收敛（时间敏感）
✅ 通用领域（新闻、评论等）
✅ 低频词较多

### 混合策略（本实验未测试）

1. **Fine-tuning**: 从pre-trained开始，小学习率微调
2. **部分frozen**: 高频词fine-tune，低频词frozen
3. **多通道**: 像CNN2那样同时用random和pre-trained

---

## 未解之谜

### 1. 为什么Pre-trained最终没有更大优势？

可能原因：
- 电影评论语言相对简单
- 训练数据足够充分
- 情感词汇在训练中频繁出现

### 2. 如果继续训练会怎样？

从曲线看Random还在上升，Pre-trained趋于平稳，可能：
- Random最终超越Pre-trained
- 或两者都开始过拟合

### 3. 不同领域会怎样？

医疗、法律等专业领域，通用预训练可能不足，需要领域特定预训练

---

## 数学附录

### Softmax导数完整推导

$$\hat{y}_i = \frac{\exp(o_i)}{\sum_k \exp(o_k)} = \frac{e^{o_i}}{Z}$$

对于 $j \neq i$:

$$\frac{\partial \hat{y}_i}{\partial o_j} = \frac{0 \cdot Z - e^{o_i} \cdot e^{o_j}}{Z^2} = -\frac{e^{o_i}}{Z} \cdot \frac{e^{o_j}}{Z} = -\hat{y}_i\hat{y}_j$$

对于 $j = i$:

$$\frac{\partial \hat{y}_i}{\partial o_i} = \frac{e^{o_i} \cdot Z - e^{o_i} \cdot e^{o_i}}{Z^2} = \hat{y}_i(1 - \hat{y}_i)$$

交叉熵梯度：

$$\frac{\partial \mathcal{L}}{\partial o_j} = -\sum_i y_i \frac{1}{\hat{y}_i}\frac{\partial \hat{y}_i}{\partial o_j}$$

代入softmax导数：

$$= -y_j(1-\hat{y}_j) + \sum_{i \neq j}y_i\hat{y}_j = -y_j + y_j\hat{y}_j + \hat{y}_j\sum_{i \neq j}y_i$$

因为 $\sum_i y_i = 1$:

$$= -y_j + \hat{y}_j(y_j + \sum_{i \neq j}y_i) = \hat{y}_j - y_j$$

### 链式法则的应用

从输出层到embedding层：

$$\frac{\partial \mathcal{L}}{\partial \mathbf{E}} = \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{z}} \cdot \frac{\partial \mathbf{z}}{\partial \mathbf{C}} \cdot \frac{\partial \mathbf{C}}{\partial \mathbf{X}} \cdot \frac{\partial \mathbf{X}}{\partial \mathbf{E}}$$

其中：
- $\frac{\partial \mathcal{L}}{\partial \mathbf{y}} = \hat{\mathbf{y}} - \mathbf{y}$
- $\frac{\partial \mathbf{y}}{\partial \mathbf{z}} = W_{fc}^T$
- $\frac{\partial \mathbf{z}}{\partial \mathbf{C}}$: max pooling梯度
- $\frac{\partial \mathbf{C}}{\partial \mathbf{X}}$: 卷积层梯度
- $\frac{\partial \mathbf{X}}{\partial \mathbf{E}}$: 索引操作

---

## 结论

1. **收敛速度**: Pre-trained明显更快（前期优势26%）
2. **最终性能**: 两者接近（差异0.43%）
3. **数据需求**: 15万样本足够Random学习
4. **应用建议**: 小数据用Pre-trained，大数据两者皆可

**核心发现**: 在电影评论这种相对简单、数据充足的任务上，随机初始化的词嵌入经过充分训练可以达到与预训练词向量相当的效果。但预训练词向量在收敛速度上有明显优势。
